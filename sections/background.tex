\chapter{Background}
\label{ch:Background}


\section{Neural Machine Translation}
\label{sec:Background:NMT}

%TODO: sources
Machine Translation (MT) is the process of using computer technology to translate text from one natural language to another. This can be achieved using different paradigms. There are three main types of machine translation systems: Rule-based Machine Translation (RBMT), Statistical Machine Translation (SMT). 

Conventional RBMT systems use pre-defined rules based on syntax, morphology and semantics, created by professional linguists. However, the key weakness of rule-based translation systems is that they require extensive lexicons and a large set of rules. 

SMT systems, on the other hand, uses a data-driven approach that utilizes statistical models derived from the analysis of bilingual and monolingual corpora. The quality of SMT output depends heavily on the size and quality of the corpora used to train the models.

Neural Machine Translation (NMT) is a subfield of SMT, which uses an artificial neural network to learn a statistical model for machine translation. Unlike traditional SMT systems, which require a pipeline of specialized components such as language model and translation model, NMT trains its statistical model end-to-end, mapping directly from an input source language to an output target language. NMT can recognize patterns in the source material to determine a context-based interpretation that can predict the likelihood of a sequence of words. They are more memory-efficient than SMT models and also have a higher accuracy, which makes them the appropriate choice for creating high-quality MT systems.

%TODO: figure, sources
The architecture of NMT models often consists of an encoder and a decoder
(Figure 1). Firstly, each word in the input sentence is fed separately into the
encoder to encode the source sentence into an internal fixed-length representation
called the context vector. This context vector contains the meaning of the sentence.
Secondly, the decoder decodes the fixed-length context vector and then predicts
the output sequence. While some types of Encoder-Decoder model used LSTM-based
approach (e.g. Sutskever et al. (2014), Luong et al. (2015b)), the others (e.g.
Luong et al. (2015a), Vaswani et al. (2017), Galassi et al. (2019)) explored the use of attention-based architectures for neural machine translation. Since in this work, I make use of models based on the Transformer architecture, next I will introduce its basic principle and components.

%TODO: figure and sources
\paragraph{Transformer architecture} 
A Transformer is an Encoder-Decoder Sequence-to-Sequence model, introduced by \cite{transformer}. Sequence-to-Sequence models consist of an Encoder and a Decoder. The Encoder takes the input sequence and maps it into a higher dimensional space, an n-dimensional matrix, that contains token embeddings for each token of the sequence, carrying context information about the other tokens in the sequence weighted by relevance. That abstract matrix is fed into the Decoder, which produces an output sequence by autoregressive sampling from the encoder matrix and its own previous values. An important feature of the Transformer architecture is its attention mechanism. The attention module looks at an input sequence and decides at each step which other parts of the sequence are important, differentially weighting the significance of each part of the input data. 

Like Recurrent Neural Networks (RNNs), Transformers are designed to handle sequential input data, such as natural language. However, unlike RNNs, Transformers can process the whole input sequence in parallel. The attention mechanism provides context for any position in the input sequence. This feature allows for more parallelization than RNNs and therefore reduces training times significantly.

% HOW in deep should I explain the Transformer and NMT
- Transformer and deep learning
- Word embeddings
- WSD (if I end up using it)
- Definitions of bias and  ambiguity
- Types of biases
- Types of languages

\section{Ambiguity and Bias in Machine Translation}
\label{sec:Background:Ambiguity_Bias}

Biases present in AI systems are an important problem stemming from cultural and historical issues present in the data from which models are learning. The developed systems in turn reinforce the present societal prejudices and old social norms, instead of mitigating them.

% “man is to computer programmer as woman is to homemaker” \parencite{bolukbasi2016man}


% For bias detection we need:
% - Challenge sets are artificially created usually small datasets that represent some gender-related issue, such as assigning the right pronoun to a specific role
% - Automatic evaluation methods needed, because the BLEU score, normally used for assessing the quality of translations, cannot judge on the occurrence of bias
% WSD: technique in natural language processing (NLP), defined as the ability to determine which meaning of word is activated by the use of word in a particular context
% QE: method for predicting the quality of a given translation rather than assessing how similar it is to a reference segment, E.g. multiple beams in Beam search: low confidence = high confidence for error in translation

