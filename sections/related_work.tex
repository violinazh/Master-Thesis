\chapter{Related Work}
\label{ch:Related_work}

% Conduct a thorough literature review: Familiarize yourself with existing research and literature related to your topic. Identify the knowledge gaps and research questions that your thesis will address. 

% - summarize the existing work
% - discuss how the present work differs from it and why this is a “step forward” (new or better)

In this chapter, we will summarize existing work on the topic of bias in the literature and will outline the differences and contributions of the present work. Previous scientific work around bias has focused on detection and mitigation of bias in NMT models using various techniques, which we will present in the following two sections.  % For more information, \citet{Savoldi_2021} provide a more extensive literature review on the topic of gender bias in MT, critically reviewing current conceptualizations of bias in light of theoretical insights from related disciplines. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bias Detection}
\label{sec:Background:Bias_Detection}

Dedicated benchmarks, evaluations, and experiments have been designed in order to assess the existence and scale of gender bias across several languages. In MT, several studies concerned themselves with pronoun translation and coreference resolution across typologically different languages. 

For example, \citet{Prates_2019} investigate pronoun translation from 12 genderless languages into English. They retrieve around 1,000 job positions from the U.S. Bureau of Labor Statistics and build synthetic examples, like the Hungarian “Ö egy mernök.” (“He/She is an engineer.”). Then they use the Google Translate API to generate translations and gather statistics about the frequency of female, male and gender-neutral pronouns in the translated output. By comparing the proportion of pronoun predictions against the real-world proportion of men and women employed in 22 sectors, they find that the MT system not only exhibits a strong tendency toward male defaults, but it also underestimates feminine frequency at a greater rate than occupation data alone suggest, failing to reproduce a real-world distribution of female workers.

Similarly, \citet{Cho_2019} extend the analysis to Korean-English, including both occupations and sentiment words such as “kind”. Since the samples are ambiguous by design, the predictions of he/she pronouns are expected to be random, but masculine pronouns seem to appear more often. \citet{Cho_2019} also make light of the fact that a higher frequency of feminine pronoun translations does not necessarily reflect a bias reduction. In some cases, it may be an indication for stereotyping, such as associating nurse more often with feminine. Therefore, while frequency count may be suitable for testing under-representational harms, it may ignore stereotyping.

Expanding beyond synthetic examples and sentiment word phrases, \citet{Gonen_2020} develop a novel technique to mine examples from real-world data to explore gender issues. They inspect the translation of natural yet ambiguous English sentences into four grammatical gender languages (Russian, German, Spanish and French) from three language families (Slavic, Germanic and Romance). Their analysis of the ratio and type of generated masculine/feminine job titles consistently proves social asymmetries, such as translating “lecturer” as masculine and “teacher” as feminine.

Furthermore, \citet{Stanovsky_2019} present the first challenge set and evaluation protocol for the analysis of gender bias in MT. Challenge sets are artificially created, usually small datasets that represent some gender-related issue, such as assigning the right pronoun to a specific role. Their purpose is to isolate the impact of gender from other factors that may affect the performance of the NMT system. Also, automatic evaluation methods for bias are needed, because the BLEU score, normally used for assessing the quality of translations, cannot judge on the occurrence of bias. 

For building the challenge set called WinoMT, \citet{Stanovsky_2019} used data introduced by two recent coreference gender-bias studies: Winogender \parencite{Rudinger_2018_coreference} and WinoBias \parencite{Zhao_2018_coreference}. It is composed of English sentences which cast participants into non-stereotypical gender roles (e.g., “The doctor asked the nurse to help \textit{her} in the operation.”). WinoMT is equally balanced between male and female genders, as well as between stereotypical and non-stereotypical gender-role assignments (e.g., a female doctor versus a female nurse). 
For assessing gender bias in translating the challenge set, \citet{Stanovsky_2019} devise an automatic gender bias evaluation method for eight target languages with grammatical gender, based on morphological analysis (e.g., the use of female inflection for the word “doctor”). They use the developed method to evaluate four popular industrial MT systems and two recent state-of-the-art academic MT models, measuring gender accuracy, difference in performance between male and female, difference in performance between stereotypical and non-stereotypical gender role assignments. An important discovery the authors made was that MT systems are faulty to the point of actually ignoring explicit feminine gender information in source English sentences. For example, MT systems produce a wrong masculine translation of the job title “baker”, although it is referred to by the female pronoun “she”.

Furthermore, \citet{MuST-SHE} develop the MuST-SHE corpus, a natural benchmark for three language pairs (English-French/Italian/Spanish). Unlike challenge sets, natural corpora quantify whether MT reduced feminine representation in real-world scenarios and whether the quality of service varies across speakers of different genders. The MuST-SHE corpus is built on  TED talks data and the samples are balanced between masculine and feminine phenomena, and incorporate two types of constructions: sentences referring to the speaker (e.g., “I was born in Mumbai”), and sentences that present contextual information to disambiguate gender (e.g., “My mum was born in Mumbai”). Because every gender-marked word in the target language is annotated in the corpus, the corpus allows for accuracy-based evaluations on gender translation for four different types of gender phenomena. However, all gender marked words are treated equally, so it is not possible to identify if the model is propagating stereotypical representations.

On another note, \citet{Hovy_2020} speculate that the existence of age and gender stylistic bias may be due to under-exposure of MT models to the writings of women and younger people. They test this assumption by automatically translating a corpus of online reviews with available metadata about users. Then, they compare this demographic information with the prediction of age and gender classifiers run on the MT output. The results prove that different commercial MT models systematically make authors appear older and male.

Another contribution to the topic of bias detection is made by \citet{roberts2020decoding}. They prove that beam search unlike sampling is skewed toward the generation of more frequent (masculine) pronouns, as it leads models to an extreme operating point that exhibits zero variability.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bias Mitigation}
\label{sec:Background:Bias_Mitigation}

To reduce the effect of gender bias, recent studies have proposed different strategies for dealing with input data, learning algorithms and model outputs. 

\citet{Stanovsky_2019} attempted to fight bias with bias in an automatically created version of WinoMT with the adjectives “handsome” and “pretty” prepended to male and female entities, respectively. The results revealed that adding a socially implied adjective (“the \textit{pretty} baker”) influences the model to choose a feminine gender word in translation. However, this is really not applicable in a real-world scenario, but is still a proof of the model’s reliance on unintended and often irrelevant cues.

% Model debiasing through metadata
\citet{Vanmassenhove_2018} build upon the assumption that demographic factors such as gender and age also influence our use of language in terms of word choices or even on the level of syntactic constructions by integrating gender information into NMT systems. They compile a large multilingual dataset on the politics domain that contains the speaker information for 20 language pairs and conduct a simple set of experiments that incorporate gender information into NMT for multiple language pairs. The outputs of the gender-informed MT models are compared with those obtained by their baseline counterparts. The results prove that providing a gender feature to an NMT system significantly improves the translation quality for some language pairs. 

Similarly, \citet{Saunders_2020_coreference} explore the use of word-level gender tags on an expanded version of WinoMT for translating coreference sentences where the reference gender label is known. They find that existing approaches overgeneralize from a gender signal, incorrectly using the same inflection for every entity in the sentence. To solve this problem, they propose a tagged-coreference adaptation approach.

Another study which attempts to debias MT models through metadata is done by \citet{Moryossef_2019}. The authors use a black-box approach to provide the missing gender information for translations without the need to train or retrain the original translation model. For this purpose, a simple construction like “she said to them” is added to the source sentence and later removed from the MT output. In doing so, they improve translation quality, as measured by BLEU score. However, these solutions require additional metadata regarding the gender of the speaker that might not always be possible to acquire.

% Debiasing word embeddings
\citet{bolukbasi2016man} and \citet{Zhao_2018_GN-GloVe} take a different approach to mitigating gender bias by debiasing word embeddings. \citet{bolukbasi2016man} propose a hard-debiasing method, based on post-processing word embeddings. This pipeline approach has some downsides, such as propagating errors and completely removing gender information from words, as well as removing valuable information pertaining to the semantic relations between words with several meanings unrelated to the bias being treated.
To improve upon this solution, \citet{Zhao_2018_GN-GloVe} propose instead a training procedure for learning gender-neutral word embeddings, called GN-GloVe, a Gender-Neutral variant of the GloVe embedding algorithm \parencite{Glove}.

% Model debiasing through debiased word embeddings
Furthermore, \citet{Escud_Font_2019} study the presence of gender bias in MT and give insight on
the impact of debiasing in such systems. For this purpose, they develop the bilingual English-Spanish Occupations test set. It consists of 1000 sentences equally distributed across genders and contains gender information in the source context as coreference. The authors propose a gender-debiased approach for NMT, in which they integrate debiased word embeddings (hard-debiased \parencite{bolukbasi2016man} or debiased using GN-GloVe \parencite{Zhao_2018_GN-GloVe}) into the NMT model. The proposed system is then evaluated on the occupations test set, showing that it learns to equalize existing biases compared to a baseline system. This verifies the hypothesis that if the translation system is gender biased, the context is disregarded, while if the system is neutral, the translation is correct when the text contains context information regarding the gender.

% Model debiasing through balanced datasets
Another possibility for mitigating gender bias is through the use of gender-balanced datasets. Gender-balanced datasets are datasets featuring an equal amount of masculine/feminine references. \citet{costa2020fine} explore this by using the GeBioToolkit \parencite{costa2019gebiotoolkit} for automatic extraction of gender-balanced multilingual data from Wikipedia biographies and fine-tune NMT models on this data. The results show that the generation of feminine forms is overall improved. However, this approach does not remove stereotyping harms, because it does not take into account the qualitative different ways in which men and women are portrayed, like the translation on the anti-stereotypical WinoMT set proves.

Another study which focuses on data modification to combat biases is by \citet{lu2020gender}. The authors mitigate bias with counterfactual data augmentation,  which inserts counterfactuals—sentences where gendered components are reversed, or countered, into the training data, alongside the originals. The results prove that this method effectively decreases gender bias while preserving accuracy, and also outperforms more traditional means of mitigating gender bias, such as word embedding debiasing. Despite that, the duplication effect this technique causes may be problematic.

% Model debiasing through architecture modification
A different approach to debiasing NMT models is centered around introducing external components into the model architecture. For example, \citet{Saunders_2020} propose to post-process the MT output with a lattice re-scoring module. This module uses a converter to create a lattice by mapping gender-marked words in the MT output to all their possible inflectional variants. All paths in the lattice are re-scored with another model, which has been gender-debiased by fine-tuning the model on an augmented dataset containing a balanced number of masculine and feminine forms. Then, the sentence with the highest probability is picked as the final output. The experiments on the WinoMT test set show an increase in gender accuracy, however, data augmentation is a demanding task, especially for complex sentences that represent a rich variety of natural gender phenomena. 

On another note, \citet{Habash_2019} and \citet{alhafni2020gender} confront the problem of unresolved gender of the speaker in Arabic with a post-processing component that re-inflects first person references into masculine/feminine forms. Similarly, Google Translate also delivers two outputs for short gender-ambiguous queries from English to Spanish \parencite{johnson2020scalable}.

A more recent study introduced the Fairslator tool \parencite{bias_taxonomy}, which detects and corrects bias in the output of any machine translator. The tool uses a formalism for describing situations in MT when the source text leaves some properties unspecified, but the target language requires the property to be specified (refer to Subsection \ref{sec:Background:Bias} for categories of properties which could lead to bias). It predicts the ambiguity and formulates human-friendly disambiguation questions for users of the type “Who is saying it?”, giving the option of selection of the property in question.

\newpage

% Conclusion and forward
Considering the above methods for bias detection and mitigation, there is currently no conclusive state-of-the-art method for preventing bias. The discussed interventions in MT tend to respond to specific aspects of the problem with modular solutions, but if and how they can be integrated within the same MT system remains unexplored. Next, we will outline how the present work differs from existing approaches and what contribution it brings to the research field of NMT.

%%% Multimodal data
% Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints \parencite{Zhao_2017}
% - study data and models associated with multilabel object classification and visual semantic role labeling
% - Findings: (a) datasets for these tasks contain significant gender bias, (b) models trained on these datasets further amplify existing bias
% - propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on Lagrangian relaxation for collective inference. 
 

%--------------------------------------------------------------------------------------------------------


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Present Work}
\label{sec:Related_work:Present_work}

The present work expands on the subject of detecting biases in NMT models by focusing on ambiguity that could lead to bias. Previous work has focused on removing bias by modifying the dataset, adjusting the model or by adding additional metadata relating to the ambiguous words. In contrast, our approach aims to detect ambiguous words in text before they lead to bias by a method of disambiguation and inspecting the diversity of translation. The developed method does not rely on context relating to the ambiguity in the form of coreference resolution, but aims to uncover unresolvable ambiguity in text. 

Since most of the available benchmarks, as presented above, are focused on ambiguity in terms of gender, this work also uses these benchmarks for evaluation, but does not limit its further application to other types of ambiguity. The main contribution of this study pertains to helping to prevent MT systems from making an unjustified assumption, which is usually the precursor of bias.

